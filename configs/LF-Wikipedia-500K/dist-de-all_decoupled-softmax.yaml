__dependency__: configs/dual-encoder.yaml, configs/LF-Wikipedia-500K/dataset.yaml
expname: dist-de-all_decoupled-softmax_exp
desc: "Training dist-de-all (distributed dual-encoder with all labels) with decoupled-softmax loss"

# Network and loss parameters
net: dist-de-all
norm_embs: False
tau: 1
neg_type: none # dist-de-all nets uses all labels as negatives, so no need to sample negatives in data loading
loss_criterion: decoupled-softmax
loss_sample: True
amp_encode: True

# Training parameters
bsz: 768
gc_bsz: 512
lr: 7.e-5
dropout: 0
eval_interval: 1